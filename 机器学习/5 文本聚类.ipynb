{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "根据词频提取出《浅谈能源文化》文章的关键字"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jieba\n",
    "import pandas as pd\n",
    "# 1、读取文章《浅谈能源文化》\n",
    "with open('../data/浅谈能源文化.txt', 'r') as f:\n",
    "    txt = f.read()\n",
    "# 2、分词\n",
    "txt = txt.split()\n",
    "words = [jieba.lcut(x) for x in txt]\n",
    "# 3、去除停用词\n",
    "with open('../data/stoplist.txt', 'r', encoding='utf-8') as f:\n",
    "    stop = f.read()\n",
    "stop = stop.split()\n",
    "words2 = [[i for i in x if i not in stop] for x in words]\n",
    "# 4、统计词频\n",
    "a = [' '.join(x) for x in words2]\n",
    "b = ' '.join(a)\n",
    "cipin = pd.Series(b.split()).value_counts()\n",
    "# 5、提取词频最多的5个词语作为文章的关键字\n",
    "cipin.index[:5]\n",
    "key_words = cipin.iloc[:5].index\n",
    "print(key_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "改进：根据词频-逆文档概率提取《浅谈能源文化》的关键字"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/字典.txt', 'r', encoding='utf-8') as f:\n",
    "    dic = f.readlines()\n",
    "print(dic[:4])\n",
    "dic2 = [i.split()[:2] for i in dic]\n",
    "print(dic2[:4])\n",
    "dic3 = {i[0]:i[1] for i in dic2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 100000\n",
    "import math\n",
    "word3 = []\n",
    "tf_idf3 = []\n",
    "tf3 = []\n",
    "idf3 = []\n",
    "for (k, w) in zip(cipin[:10], cipin[:10].index):\n",
    "    tf = k\n",
    "    idf = math.log(n/(int(dic3.get(w, 0))+1))\n",
    "    tf_idf = tf*idf\n",
    "    word3.append(w)\n",
    "    tf_idf3.append(tf_idf)\n",
    "    tf3.append(tf)\n",
    "    idf3.append(idf)\n",
    "    print(f'词语 {w} 在文章中的词频为{k}, 在语料库中出现的次数为{dic3.get(w, 0)}，tf-idf是{tf_idf}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.DataFrame({'words':word3, 'tf':tf3, 'idf':idf3, 'tf-idf':tf_idf3})\n",
    "data.sort_values(by='tf-idf', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 文本聚类"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对手机评论数据进行聚类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "f = open('../data/vivo数据.csv', encoding='utf-8')\n",
    "data = pd.read_csv(f)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "content = data['content']\n",
    "content.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "分词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jieba\n",
    "content = content.apply(jieba.lcut)\n",
    "content.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "去除停用词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/stoplist.txt', 'r', encoding='utf-8') as f:\n",
    "    stop = f.read()\n",
    "stop = stop.split()\n",
    "content = content.apply(lambda x: [i for i in x if i not in stop])\n",
    "content.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "转换为文本-词条矩阵"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = content.apply(lambda x: ' '.join(x))\n",
    "y = data['lab']\n",
    "print(x.head(), y.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "cv = CountVectorizer()\n",
    "x_data = cv.fit_transform(x)\n",
    "cv.vocabulary_  # 字典\n",
    "cv.get_feature_names()  # 词条\n",
    "x_data.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用KMeans聚类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "model = KMeans(n_clusters=5).fit(x_data)\n",
    "model.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pre = model.labels_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "评价聚类效果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import silhouette_score\n",
    "# help(silhouette_score)\n",
    "silhouette_score(x_data, y_pre)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(3, 9):\n",
    "    model = KMeans(n_clusters=i).fit(x_data)\n",
    "    y_pre = model.labels_\n",
    "    print(f'{i} {silhouette_score(x_data, y_pre)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 情感分析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 文本分类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "      <th>0</th>\n",
       "      <th>商业秘密的秘密性那是维系其商业价值和垄断地位的前提条件之一</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>南口阿玛施新春第一批限量春装到店啦         春暖花开淑女裙、冰蓝色公主衫  ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>带给我们大常州一场壮观的视觉盛宴</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>有原因不明的泌尿系统结石等</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>23年从盐城拉回来的麻麻的嫁妆</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>感到自减肥、跳减肥健美操、</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   1  0                      商业秘密的秘密性那是维系其商业价值和垄断地位的前提条件之一\n",
       "0  2  1  南口阿玛施新春第一批限量春装到店啦         春暖花开淑女裙、冰蓝色公主衫  ...\n",
       "1  3  0                                   带给我们大常州一场壮观的视觉盛宴\n",
       "2  4  0                                      有原因不明的泌尿系统结石等\n",
       "3  5  0                                    23年从盐城拉回来的麻麻的嫁妆\n",
       "4  6  0                                      感到自减肥、跳减肥健美操、"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "f = open('G:/CDO/NLP/自然语言处理技术与应用/自然语言处理技术/06.数据/message80W1.csv', encoding='utf-8')\n",
    "data = pd.read_csv(f)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         南口阿玛施新春第一批限量春装到店啦         春暖花开淑女裙、冰蓝色公主衫  ...\n",
       "1                                          带给我们大常州一场壮观的视觉盛宴\n",
       "2                                             有原因不明的泌尿系统结石等\n",
       "3                                           23年从盐城拉回来的麻麻的嫁妆\n",
       "4                                             感到自减肥、跳减肥健美操、\n",
       "                                ...                        \n",
       "799994                                    助排毒缓解痛经预防子宫肌瘤&amp\n",
       "799995                                     这是今年首次启动I级防台应急响应\n",
       "799996                                       丽江下飞机时迎接我们的是凉风\n",
       "799997                              费了半天劲各种找关系终于联系上心仪公司的内部人\n",
       "799998                                      是汉奸还是被强奸自己对号入座吧\n",
       "Name: 商业秘密的秘密性那是维系其商业价值和垄断地位的前提条件之一, Length: 799999, dtype: object"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "content = data['商业秘密的秘密性那是维系其商业价值和垄断地位的前提条件之一']\n",
    "content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "expected string or bytes-like object",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-19d19c362de4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mjieba\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mcontent\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcontent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjieba\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcut\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mcontent\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\pandas\\core\\series.py\u001b[0m in \u001b[0;36mapply\u001b[1;34m(self, func, convert_dtype, args, **kwds)\u001b[0m\n\u001b[0;32m   3846\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3847\u001b[0m                 \u001b[0mvalues\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3848\u001b[1;33m                 \u001b[0mmapped\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap_infer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconvert\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconvert_dtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3849\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3850\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mSeries\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\lib.pyx\u001b[0m in \u001b[0;36mpandas._libs.lib.map_infer\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m<ipython-input-14-19d19c362de4>\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mjieba\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mcontent\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcontent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjieba\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcut\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mcontent\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\anaconda\\lib\\site-packages\\jieba\\__init__.py\u001b[0m in \u001b[0;36mcut\u001b[1;34m(sentence, cut_all, HMM)\u001b[0m\n\u001b[0;32m    246\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    247\u001b[0m         \u001b[0mre_han\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mre_skip\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mre\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"([\\u4E00-\\u9FA5a-zA-Z0-9+#&\\._]+)\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mre\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mU\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mre\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"(\\r\\n|\\s)\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mre\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mU\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 248\u001b[1;33m     \u001b[0mblocks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mre_han\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    249\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mcut_all\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    250\u001b[0m         \u001b[0mcut_block\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m__cut_all\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: expected string or bytes-like object"
     ]
    }
   ],
   "source": [
    "import jieba\n",
    "content = content.apply(lambda x: list(jieba.cut(x)))\n",
    "content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [指纹, 闹着玩, vivox7plus, 太, 老火, 18, 号, 买, 部, 只到, ...\n",
       "1                 [久, 部坏机, 右手, 指纹, 解锁, 不行, 左手, 解锁, 成功]\n",
       "2    [第一次, 京东, 买手机, 买, 完, 后悔, 物流, 慢, 关键, 包装, 简单, 收到...\n",
       "3    [手机, 刚用, 天, 屏幕, 不好, 京东, 买, vivo, 实体店, 退换货, 京东,...\n",
       "4    [25, 说, 到手, 26, 拿到, 手, 客服, 解决不了, 只会, 说, 抱歉, 手机...\n",
       "Name: content, dtype: object"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('../data/stoplist.txt', 'r', encoding='utf-8') as f:\n",
    "    stop = f.read()\n",
    "stop = stop.split()\n",
    "stop = stop + ['\\n', ' ']\n",
    "content = content.apply(lambda x: [i for i in x if i not in stop])\n",
    "content.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0     指纹 闹着玩 vivox7plus 太 老火 18 号 买 部 只到 一部 hellip h...\n",
      "1                            久 部坏机 右手 指纹 解锁 不行 左手 解锁 成功\n",
      "2     第一次 京东 买手机 买 完 后悔 物流 慢 关键 包装 简单 收到 手机 装 手机 盒子 ...\n",
      "3     手机 刚用 天 屏幕 不好 京东 买 vivo 实体店 退换货 京东 拿走 做 鉴定 太 麻...\n",
      "4     25 说 到手 26 拿到 手 客服 解决不了 只会 说 抱歉 手机 想象 中要 厚 指纹 ...\n",
      "5                                  触屏 不灵 反应迟钝 app 卡死 再评\n",
      "6     拿到 手 惊艳 iPhone 用户 屏幕 颜色 显示 别扭 开 护眼 模式 舒服 地方 说 ...\n",
      "7     一星 致敬 vivo 自营 碎 屏险 三个 供应商 客服 三个 说法 态度 高冷 发货 时间...\n",
      "8     先说 说 一颗 星 京东 发货 第一批 10 分钟 全款 预定 拖 星期 发货 说好 送 半...\n",
      "9     手机 边框 边框 拐角 手机 屏幕 边框 合拢 缝隙 闪光灯 反光 描述 心情 好多天 后悔...\n",
      "10    完 完 中奖 缝 价位 品控 无语 手机 真的 好看 真 瓶 没得说 满意 屏幕 清晰 xp...\n",
      "Name: content, dtype: object 0    1\n",
      "1    1\n",
      "2    1\n",
      "3    1\n",
      "4    1\n",
      "Name: lab, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "x = content.apply(lambda x: ' '.join(x))\n",
    "y = data['lab']\n",
    "print(x.head(11), y.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0]], dtype=int64)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "cv = CountVectorizer()\n",
    "x_data = cv.fit_transform(x)\n",
    "cv.vocabulary_  # 字典\n",
    "cv.get_feature_names()  # 词条\n",
    "x_data.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_data.toarray(), y, test_size=0.2, random_state=123)\n",
    "model = GaussianNB().fit(x_train, y_train)\n",
    "y_pre = model.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  9   0   3   1   8]\n",
      " [  1   0   0   0   2]\n",
      " [  0   0   3   0   7]\n",
      " [  0   0   0   9   3]\n",
      " [ 17  10  25  16 367]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          1       0.33      0.43      0.38        21\n",
      "          2       0.00      0.00      0.00         3\n",
      "          3       0.10      0.30      0.15        10\n",
      "          4       0.35      0.75      0.47        12\n",
      "          5       0.95      0.84      0.89       435\n",
      "\n",
      "avg / total       0.88      0.81      0.84       481\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "print(confusion_matrix(y_test, y_pre))\n",
    "print(classification_report(y_test, y_pre))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 使用FastText分类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=.2, random_state=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../tmp/train.txt', 'w', encoding='utf-8') as f:\n",
    "    for i in x_train.index:\n",
    "        f.write(x_train[i]+ ' __label__' + str(y_train[i]) +'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fastText\n",
    "classifier = fastText.train_supervised(r'..\\tmp\\train.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['</s>', '手机', '屏幕', '不错', '摄像头', '指纹', '感觉', '屏', 'vivo', '买', '说', '喜欢']"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.get_words()[:12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['__label__5', '__label__1', '__label__3', '__label__4', '__label__2']"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.get_labels()[:12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pre = classifier.predict(list(x_test))\n",
    "y_pre = [int(x[-1]) for x in y_pre[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  0   4   0   0  17]\n",
      " [  0   0   0   0   3]\n",
      " [  0   0   0   0  10]\n",
      " [  0   0   0   0  12]\n",
      " [  0   0   0   0 435]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          1       0.00      0.00      0.00        21\n",
      "          2       0.00      0.00      0.00         3\n",
      "          3       0.00      0.00      0.00        10\n",
      "          4       0.00      0.00      0.00        12\n",
      "          5       0.91      1.00      0.95       435\n",
      "\n",
      "avg / total       0.82      0.90      0.86       481\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\45543\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "y_test = list(y_test)\n",
    "print(confusion_matrix(y_test, y_pre))\n",
    "print(classification_report(y_test, y_pre))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
